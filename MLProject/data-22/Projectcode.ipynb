{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE GENSIM DOC2VEC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "#READING A DATA FILE (TAGS range from 0 to len(data))\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = gensim.utils.simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "\n",
    "#THIS CREATES A TRAINING CORPUS\n",
    "train_corpus = list(read_corpus(\"train.txt\"))\n",
    "#Create Validation corpus\n",
    "validation_corpus = list(read_corpus(\"validation.txt\"))\n",
    "#text corpus\n",
    "test_corpus = list(read_corpus(\"test.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSTANTIATE THE GENSIM DOC2VEC MODEL\n",
    "#CREATE VOCABULARY\n",
    "#TRAIN THE GENSIM MODEL USING train_corpus\n",
    "#https://radimrehurek.com/gensim/auto_examples/tutorials/run_doc2vec_lee.html\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=50)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE VECTORS for a specific training instance in train_corpus using infer_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1305031  -0.05924479 -0.6960198   0.11471944 -0.01019582 -0.06324498\n",
      " -0.36004782 -0.48733127 -0.09323093 -0.3469856   0.40649232 -0.07180716\n",
      " -0.13098419  0.12916279  0.0580754   0.15191723  0.22735171  0.27661857\n",
      "  0.02000582  0.33443126  0.02258343  0.0449286   0.09695464  0.593966\n",
      " -0.23650116  0.00817174 -0.5920629  -0.08337425  0.3251718  -0.00323302\n",
      "  0.09217398 -0.2983118  -0.6494542   0.00921688 -0.09935842  0.4955143\n",
      "  0.20736198 -0.23427123  0.40181515 -0.2439776 ]\n"
     ]
    }
   ],
   "source": [
    "vector = model.infer_vector(['the', 'welcome', 'housekeeping', 'room'])\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack all the vectors using numpy.vstack() to obtain X (features for classifier)\n",
    "import numpy as np\n",
    "features = np.vstack([model.infer_vector(doc.words) for doc in train_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the labels in trainlabels.txt to obtain Y\n",
    "y_labels = np.loadtxt(\"trainlabels.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stack all the vectors using numpy.vstack() to obtain X (features for classifier)\n",
    "validation_features = np.vstack([model.infer_vector(doc.words) for doc in validation_corpus])\n",
    "test_features = np.vstack([model.infer_vector(doc.words) for doc in test_corpus])\n",
    "#Read the labels in validatelabels.txt to obtain V\n",
    "v_labels = np.loadtxt('validationlabels.txt', dtype=int)\n",
    "#Read the labels in testlabels.txt to obtain t\n",
    "t_labels = np.loadtxt('testlabels.txt', dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNET,LR,RF on train features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "NNet = MLPClassifier().fit(features, y_labels)\n",
    "LR = LogisticRegression().fit(features, y_labels)\n",
    "RF = RandomForestClassifier().fit(features, y_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNet perfrmace on Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6373862433862434\n",
      "Precision_Score: 0.6551282051282051\n",
      "Recall_Score: 0.6333333333333334\n",
      "F1 Score: 0.6458603425559947\n",
      "Precision_Score: 0.6482983682983682\n",
      "Recall_Score: 0.65\n",
      "F1 Score: 0.6814814814814814\n",
      "Precision_Score: 0.6512820512820513\n",
      "Recall_Score: 0.7166666666666667\n"
     ]
    }
   ],
   "source": [
    "#1.NNet on validation data\n",
    "EvluationMetrics =  ['f1', 'precision', 'recall']\n",
    "LRates = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "validationScores = list()\n",
    "for lr in LRates:\n",
    "    NNet = MLPClassifier(learning_rate=lr)\n",
    "    NNet_Per = cross_validate(NNet,validation_features, v_labels, cv=5, \n",
    "                                scoring=EvluationMetrics)\n",
    "    validationScores.append(NNet_Per)\n",
    "for score in validationScores:\n",
    "    print('F1 Score: '+str(score['test_f1'].mean()))\n",
    "    print('Precision_Score: ' +str(score['test_precision'].mean()))\n",
    "    print('Recall_Score: '+str(score['test_recall'].mean()))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest perfrmace on Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6373862433862434\n",
      "Precision_Score: 0.6551282051282051\n",
      "Recall_Score: 0.6333333333333334\n",
      "F1 Score: 0.6458603425559947\n",
      "Precision_Score: 0.6482983682983682\n",
      "Recall_Score: 0.65\n",
      "F1 Score: 0.6814814814814814\n",
      "Precision_Score: 0.6512820512820513\n",
      "Recall_Score: 0.7166666666666667\n",
      "F1 Score: 0.5659654185969976\n",
      "Precision_Score: 0.625079365079365\n",
      "Recall_Score: 0.5333333333333334\n",
      "F1 Score: 0.5893333333333333\n",
      "Precision_Score: 0.6044871794871794\n",
      "Recall_Score: 0.5833333333333333\n",
      "F1 Score: 0.5806493506493507\n",
      "Precision_Score: 0.6202564102564103\n",
      "Recall_Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "NE = [30,40,50]\n",
    "for estimator in NE:\n",
    "    RF = RandomForestClassifier(n_estimators=estimator,criterion='gini').fit(features, y_labels)\n",
    "    RF_Per = cross_validate(RF,validation_features, v_labels, cv=5, \n",
    "                                scoring=EvluationMetrics)\n",
    "    validationScores.append(RF_Per)\n",
    "\n",
    "for score in validationScores:\n",
    "    print('F1 Score: '+str(score['test_f1'].mean()))\n",
    "    print('Precision_Score: ' +str(score['test_precision'].mean()))\n",
    "    print('Recall_Score: '+str(score['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression perfrmace on Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.6438431577861864\n",
      "Precision_Score: 0.6388591800356507\n",
      "Recall_Score: 0.6666666666666667\n",
      "F1 Score: 0.6661523783562764\n",
      "Precision_Score: 0.6869477581242288\n",
      "Recall_Score: 0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "classRatio = [0,1]\n",
    "lrValidationScores = list()\n",
    "for ratio in classRatio:\n",
    "    RF = LogisticRegression(l1_ratio=ratio, penalty='elasticnet', solver='saga')\n",
    "    RF_Per = cross_validate(RF,validation_features, v_labels, cv=5, \n",
    "                                scoring=EvluationMetrics)\n",
    "    lrValidationScores.append(RF_Per)\n",
    "for score in lrValidationScores:\n",
    "    print('F1 Score: '+str(score['test_f1'].mean()))\n",
    "    print('Precision_Score: ' +str(score['test_precision'].mean()))\n",
    "    print('Recall_Score: '+str(score['test_recall'].mean()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNet perfrmace on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7257575757575758\n",
      "Precision_Score: 0.7271464646464647\n",
      "Recall_Score: 0.7393939393939395\n",
      "F1 Score: 0.6877846790890269\n",
      "Precision_Score: 0.6926975638740345\n",
      "Recall_Score: 0.7045454545454546\n",
      "F1 Score: 0.7119047619047619\n",
      "Precision_Score: 0.7059376571141277\n",
      "Recall_Score: 0.740909090909091\n"
     ]
    }
   ],
   "source": [
    "EvluationMetrics =  ['f1', 'precision', 'recall']\n",
    "LRates = [\"constant\", \"invscaling\", \"adaptive\"]\n",
    "validationScores = list()\n",
    "for lr in LRates:\n",
    "    NNet = MLPClassifier(learning_rate=lr)\n",
    "    NNet_Per = cross_validate(NNet,test_features, t_labels, cv=5, \n",
    "                                scoring=EvluationMetrics)\n",
    "    validationScores.append(NNet_Per)\n",
    "for score in validationScores:\n",
    "    print('F1 Score: '+str(score['test_f1'].mean()))\n",
    "    print('Precision_Score: ' +str(score['test_precision'].mean()))\n",
    "    print('Recall_Score: '+str(score['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forst perfrmace on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7257575757575758\n",
      "Precision_Score: 0.7271464646464647\n",
      "Recall_Score: 0.7393939393939395\n",
      "F1 Score: 0.6877846790890269\n",
      "Precision_Score: 0.6926975638740345\n",
      "Recall_Score: 0.7045454545454546\n",
      "F1 Score: 0.7119047619047619\n",
      "Precision_Score: 0.7059376571141277\n",
      "Recall_Score: 0.740909090909091\n",
      "F1 Score: 0.542855320118478\n",
      "Precision_Score: 0.6023448773448774\n",
      "Recall_Score: 0.5121212121212121\n",
      "F1 Score: 0.6512987012987013\n",
      "Precision_Score: 0.6778554778554777\n",
      "Recall_Score: 0.6363636363636364\n",
      "F1 Score: 0.5843603455368162\n",
      "Precision_Score: 0.6814141414141414\n",
      "Recall_Score: 0.55\n"
     ]
    }
   ],
   "source": [
    "NE = [30,40,50]\n",
    "for estimator in NE:\n",
    "    RF = RandomForestClassifier(n_estimators=estimator,criterion='gini').fit(features, y_labels)\n",
    "    RF_Per = cross_validate(RF,test_features, t_labels, cv=5, \n",
    "                                scoring=EvluationMetrics)\n",
    "    validationScores.append(RF_Per)\n",
    "\n",
    "for score in validationScores:\n",
    "    print('F1 Score: '+str(score['test_f1'].mean()))\n",
    "    print('Precision_Score: ' +str(score['test_precision'].mean()))\n",
    "    print('Recall_Score: '+str(score['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression perfrmace on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.7028706945228684\n",
      "Precision_Score: 0.6968831168831169\n",
      "Recall_Score: 0.7212121212121212\n",
      "F1 Score: 0.7159762845849802\n",
      "Precision_Score: 0.7240026640026639\n",
      "Recall_Score: 0.7196969696969697\n"
     ]
    }
   ],
   "source": [
    "classRatio = [0,1]\n",
    "lrValidationScores = list()\n",
    "for ratio in classRatio:\n",
    "    RF = LogisticRegression(l1_ratio=ratio, penalty='elasticnet', solver='saga')\n",
    "    RF_Per = cross_validate(RF,test_features, t_labels, cv=5, \n",
    "                                scoring=EvluationMetrics)\n",
    "    lrValidationScores.append(RF_Per)\n",
    "for score in lrValidationScores:\n",
    "    print('F1 Score: '+str(score['test_f1'].mean()))\n",
    "    print('Precision_Score: ' +str(score['test_precision'].mean()))\n",
    "    print('Recall_Score: '+str(score['test_recall'].mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, three classification algorithms were compared in a binary classification job.\n",
    "When compared to Random forest and logistic regression classifiers, the MLP Classifier with the adjustable learning rate performs better.\n",
    "More parameters can be experimented with to improve the performance of the classifiers using techniques such as GridSearch CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
